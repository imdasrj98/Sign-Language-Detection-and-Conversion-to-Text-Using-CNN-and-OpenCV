# Sign-Language-Detection-and-Conversion-to-Text-Using-CNN-and-OpenCV
The project focuses on translating American Sign Language into text using CNN model and OpenCV library of python. The training and testing of the model will done by classical convolutional neural network and then for real time application OpenCV will be used to detect the hand gesture and then the final prediction will be made by the CNN model.
# Goals
1.  Building a classification model using CNN to classify the sign language into alphabet and other classes with maximum accuracy possible.
2.  Detecting the hand gesture using OpenCV
3.  Making real time prediction using the classification model
4.  Convert it into text for furthur use.
# Dataset
The data set is a collection of images of alphabets from the American Sign Language, separated in 29 folders which represent the various classes. The training data set contains 87,000 images which are 200x200 pixels. There are 29 classes, of which 26 are for the letters A-Z and 3 classes for SPACE, DELETE and NOTHING. These 3 classes are very helpful in real time applications, and classification. The test data set contains a mere 29 images, to encourage the use of real world test images. The Data is collected from kaggle. Link : https://www.kaggle.com/grassknoted/asl-alphabet
